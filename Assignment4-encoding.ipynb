{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f0f62e",
   "metadata": {},
   "source": [
    "# Assignment 4: Encoding models (10 pts in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchextractor for facilitating feature extraction\n",
    "!pip install torchextractor\n",
    "\n",
    "# Import packages\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torchextractor as tx\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!gdown 1q5mPgOpEsWx4x_FeuZWu-8HfsdGr-aLS\n",
    "# Extract the dataset and remove the tar file\n",
    "!mkdir -p natural_scenes_demo && tar -xzf natural_scenes_demo.tar.gz -C natural_scenes_demo\n",
    "!rm natural_scenes_demo.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590df98",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "This dataset contains only one subject, who viewed 1000 images in the fMRI scanner. The images are naturalistic images. The fMRI data has been preprocessed, and voxel-wise BOLD responses has been estimated. The voxel-wise BOLD responses are stored in the file 'response_data.parquet', with rows representing voxels and columns representing images. The file 'metadata.csv' records detailed information on each voxel. \n",
    "\n",
    "The file 'stimulus_data.csv' records detailed information on each image. The image files are located at the folder 'stimulus_set'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa81664",
   "metadata": {},
   "source": [
    "In this assignment, you will fit an encoding model with AlexNet activations to BOLD responses in the region PPA.\n",
    "\n",
    "The BOLD responses in PPA have been extracted for you with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f67c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load voxel metadata\n",
    "df = pd.read_csv('natural_scenes_demo/metadata.csv')\n",
    "\n",
    "# Select voxels in PPA with reliability > 0.1\n",
    "voxel_ids = df.loc[\n",
    "    (df['roi_name'] == 'PPA') & (df['voxel_reliability'] > 0.1),\n",
    "    'voxel_id'\n",
    "].to_numpy()\n",
    "\n",
    "# Load voxel-wise BOLD responses\n",
    "data = pd.read_parquet('natural_scenes_demo/response_data.parquet')\n",
    "data = data.set_index('voxel_id')\n",
    "bold_responses = data.loc[voxel_ids].to_numpy()  # shape: (num_voxels, num_images)\n",
    "print(f'Number of voxels in PPA with reliability > 0.1: {len(voxel_ids)}')\n",
    "print(f'BOLD responses shape: {bold_responses.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee09049",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (0.5 pts):\n",
    "How is reliablity of BOLD responses generally calculated? Why do we want to remove voxels with low reliability?\n",
    "\n",
    "Write your answer here:\n",
    "> **Answer:**\n",
    "> \n",
    ">   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644cf6d",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (2 pt):\n",
    "Exact layer activations of 1000 images from the last convolutional layer (after ReLU) in Alexnet.\n",
    "_Hint: Make sure the layer you want is properly indexed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "# flatten the activations into vectors\n",
    "activations_conv5_flat = activations_conv5.reshape(len(activations_conv5), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6047a3",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (0.5 pt):\n",
    "You should have extracted 43264 features from each image in the data set from conv5. Now randomly split 80% of the data as the training set and 20% as the testing set. \\\n",
    "_Hint: Use `train_test_split` from sklearn. Set random_state=42 so the split are reproducible_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a048fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800a2be",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pt):\n",
    "Now you want to do dimension reduction by performing PCA on the 43264 features and reduce it to 50 dimensions. \\\n",
    "\n",
    "Why do you want to do dimension reduction? Should you perform PCA only on the training dataset or on the whole dataset, and why?\n",
    "\n",
    "Write your answer here:\n",
    "> **Answer:**\n",
    "> \n",
    ">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd22c2b",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pt):\n",
    "Perform PCA to reduce the dimension to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f211a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff0279",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pt):\n",
    "Build encoding models by conducting voxel-wise Ridge regression and predicting on the test dataset. \\\n",
    "_Hint: Use `Ridge` from sklearn. Set the alpha = 1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea814a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code below\n",
    "\n",
    "# ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# fit the model\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d866",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pt):\n",
    "Evaluate the performance of your encoding model by computing the correlation coefficient. Get a summary score for the ROI (i.e., PPA) by calculating the _median_ of all voxels. \\\n",
    "_Hint: Use np.corrcoef to compute correlation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "\n",
    "# compute r for each voxel\n",
    "r_scores = ...\n",
    "\n",
    "# compute the median r across voxels\n",
    "median_r = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f383e7",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (3 pt):\n",
    "Now you will wrap up all those steps above into one code cell, and put the encoding models under a _two-folds_ cross-validation scheme. Note that to get the final performance evaluation, concatenate each fold into one big data and compute the correlation.Output the median r of all voxels.\\\n",
    "_Hint: Use `KFold` to split folds. Remember to set random_state=42 for reproducibility_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
