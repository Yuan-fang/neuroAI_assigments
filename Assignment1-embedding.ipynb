{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c13e48",
   "metadata": {},
   "source": [
    "# Representation, space and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the gensim library\n",
    "!pip install gensim\n",
    "\n",
    "# Import packages\n",
    "import gensim.downloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1338d",
   "metadata": {},
   "source": [
    "You will load a small pre-trained word-embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb101cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe model\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b72b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-16588244.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the embedding vector for the word \"woman\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwoman_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the embedding vector for the word \"woman\"\n",
    "woman_vector = model['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1729b8",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (0.5 pt):\n",
    "Get the dimension of this embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3dbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbd978",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (0.5 pt):\n",
    "Get embeddings for the words “queen”, “uncle” and “tree.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a48d4b",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (2 pts):\n",
    "Compute the Euclidean distance between the embeddings of the following word pairs: (1) \"woman\" and \"queen\", (2) \"woman\" and \"uncle\", and (3) \"woman\" and \"tree\". \\\n",
    "_Hint: use `np.linalg.norm` to compute L2 norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4006221",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (2 pts):\n",
    "Compute the cosine distance between the embeddings of the following word pairs: (1) \"woman\" and \"queen\", (2) \"woman\" and \"uncle\", and (3) \"woman\" and \"tree\". \\\n",
    "_Hint: use `@` to compute dot product_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e770ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57febd83",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pt):\n",
    "Compute the cosine similarity between the embeddings of the following word pairs: (1) \"woman\" and \"queen\", (2) \"woman\" and \"uncle\", and (3) \"woman\" and \"tree\". (1 pt)\n",
    "_Hint: compute consine similarity from consine distance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080027e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1f082",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (1 pts):\n",
    "Combining the above embedding examples, explain why cosine distance is generally preferred over Euclidean distance when comparing embeddings. \n",
    "\n",
    "Write your answer here, right below this line:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82294b06",
   "metadata": {},
   "source": [
    "### Get embedding of an image from a vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfeac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the image file\n",
    "!wget -O surfing.png \"https://drive.google.com/uc?export=download&id=1drpMOkT81nX2GwlvOvjRvs1-YwKlEIQs\" -q\n",
    "\n",
    "# Display the image\n",
    "img = Image.open(\"surfing.png\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ec3b4",
   "metadata": {},
   "source": [
    "Load in the AlexNet model.\n",
    "Visualize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8be4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on the version of torchvision, you might need to use:\n",
    "# model = models.alexnet(pretrained=True) # for torchvision versions < 0.13\n",
    "model = models.alexnet(weights='IMAGENET1K_V1') # for torchvision versions >= 0.13\n",
    "\n",
    "# Make the model in evaluation mode, so it does not update weights\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b495161",
   "metadata": {},
   "source": [
    "### Demo on how to get activations (i.e., embeddings) from a specific layer of the model\n",
    "1. Read in the image\n",
    "2. Preprocess the image to fit the input size of the model\n",
    "3. Define the hook function (a callback function) and attach it to a specific layer\n",
    "4. Forward-pass the image to obtain activations (embeddings) from the specified layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Open the image file\n",
    "input_img = Image.open(\"surfing.png\")\n",
    "\n",
    "# Preprocess the image to fit the input size of the model\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply the preprocessing to the input image\n",
    "input_tensor = preprocess(input_img)\n",
    "\n",
    "# Add a batch dimension so the model can process the image\n",
    "x = input_tensor.unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "\n",
    "\n",
    "# Define a dictionary to store the activations\n",
    "activations = {}\n",
    "\n",
    "# Define the hook function. This function will be called during the forward pass.\n",
    "def hook_fn(name):\n",
    "    def _hook(module, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return _hook\n",
    "\n",
    "# Register the hook on the fc7 layer\n",
    "handle = model.classifier[4].register_forward_hook(hook_fn(\"fc7\")) \n",
    "\n",
    "# Run the model with the input image to trigger the hook\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "# Remove the hook in case you want to register another one later\n",
    "handle.remove()\n",
    "\n",
    "# Get the embedding from the fc7 layer\n",
    "emb = activations[\"fc7\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09095595",
   "metadata": {},
   "source": [
    "Next, you will download two other images and get their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bee857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download two other image files\n",
    "!wget -O img01.png \"https://drive.google.com/uc?export=download&id=1Y5nIJ_0VOHTWWx2LMpE-oiJuwXoWjU-N\" -q\n",
    "!wget -O img02.png \"https://drive.google.com/uc?export=download&id=1zvHbYiO-6N4kma-KLi0XBYSwE2niadjQ\" -q\n",
    "\n",
    "# Display the first image\n",
    "display(Image.open(\"img01.png\"))\n",
    "\n",
    "# Display the second image\n",
    "display(Image.open(\"img02.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec75710",
   "metadata": {},
   "source": [
    "#### ✏️ Do it yourself (3 pts):\n",
    "Get the embeddings of 'img01' and 'img02' from the _fc7_ layer of Alexnet (1.5 pts) and compute the cosine similarity between the embeddings of image pairs: (1) \"surfing\" and \"img01\", (2) \"surfing\" and \"img02\" (1.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3682825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
